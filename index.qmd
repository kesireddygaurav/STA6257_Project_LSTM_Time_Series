---
title: "LSTM Time Series"
author: "Gaurav Kesireddy, Fariha Moomtaheen, Nitul Singha"
date: '`r Sys.Date()`'
format:
  html:
    code-fold: true
course: STA 6257 - Advance Statistical Modeling
bibliography: references.bib # file contains bibtex for references
#always_allow_html: true # this allows to get PDF with HTML features
editor: 
  markdown: 
    wrap: 72
---

## Introduction

Recurrent Neural Networks (RNNs) are one of the most popular data-driven
approaches used for time-series prediction. RNNs benefit from feedback
loops which makes the output of RNN in one time step to be seen as an
input in the subsequent one. Having information in a sequential form,
the input of the RNN in each time step includes the information of the
corresponding time step in the sequence and the previous information
provided by a feedback loop[@karevan2020transductive]. A RNN maintains a
memory based on history information, which enables the model to predict
the current output conditioned on long distance
features[@huang2015bidirectional].Recurrent networks can in principle
use their feedback connections to store representations of recent input
events in form of activation's ("short-term memory", as opposed to
"long-term memory embodied by slowly changing weights). The most widely
used algorithms for learning what to put in short-term memory, however,
take too much time or do not work well at all, especially when minimal
time lags between inputs and corresponding teacher signals are long.
With conventional"Back-Propagation Through Time" or "Real-Time Recurrent
Learning", error signals "flowing backwards in time" tend to either (1)
blow up or (2) vanish: the temporal evolution of the back-propagated
error exponentially depends on the size of the weights. Case (1) may
lead to oscillating weights, while in case (2) learning to bridge long
time lags takes a prohibitive amount of time, or does not work at all.

Recurrent Neural Networks, often shortened to RNNs, are a class of
neural networks which exhibit temporal behavior due to directed
connections between units of an individual layer. Recurrent neural

networks maintain a hidden vector **h**, which is updated at time step
*t* as follows:

$$ h_t = tanh(Wh_{t-1}+Ix_t)$$ where tanh is the hyperbolic tangent
function. **W** is the recurrent weight matrix and **I** is a projection
matrix. The hidden state **h** is used to make a prediction

$$y_t = softmax(Wh_{t-1})$$ where softmax provides a normalized
probability distribution over the possible classes, $\sigma$ is the
logistic sigmoid function and **W** is a weight matrix. By using **h**
as the input to another RNN, we can stack RNNs, creating deeper
architectures[@karim2017lstm].
$$h^l_t = \sigma(Wh^l_{t-1} + Ih^{l-1}_t)$$

Long Short-Term Memory(LSTM), a novel recurrent network architecture in
conjunction with an appropriate gradient based learning algorithm comes
up as a remedy to solve this problem. It can learn to bridge time
intervals in excess of 1000 steps even in case of noisy, in-compressible
input sequences, without loss of short term time lag capabilities. This
is achieved by an efficient, gradient-based algorithm for an
architecture enforcing constant error flow through internal states of
special units[@hochreiter1997long]. Recurrent neural networks with Long
Short-term Memory have emerged as an effective and scalable model for
several learning problems related to sequential data. The central idea
behind the LSTM architecture is a memory cell which can maintain its
state over time, and non-linear gating units which regulate the
information flow into and out of the cell. Most modern studies
incorporate many improvements that have been made to the LSTM
architecture since its original formulation.However, LSTM's are now
applied to many learning problems which differ significantly in scale
and nature from the problems that these improvements were initially
tested on[@greff2016lstm].

Due to information explosion in recent years, a large amount of data has
become available online and manually analyzing this data is not
feasible. Various supervised and unsupervised machine learning
techniques have been developed to automatically gather and analyze data
and make predictions. Deep learning techniques have become the hottest
tool delivering state-of-the art results in areas as diverse as computer
vision, speech recognition and NLP problems. But deep learning
techniques also present a unique set of challenges. The performance of
deep learning models depends upon a number of parameters called
hyper-parameters. Hyper-parameters are manually set training variables
which are determined before training the model. Some automatic
techniques for tuning hyper-parameters have been proposed but these
methods don't provide insight to the end-users about the interactions
between different hyper-parameters and their relative importance. For
the deep learning model used in this paper, namely LSTM, the important
hyper-parameters are activation function (sigmoid, tanh, softmax etc.),
optimizer (Adam, Adadelta, RMSprop etc.), batch size, number of epochs,
number of hidden layers etc. LSTM can be stateful or
stateless[@yadav2020optimizing].

A critical area of machine learning is Time Series forecasting, as
various forecasting problems contain a time component. It is one of the
most applied data science technique in business, supply chain
management, and production. A series of observations taken
chronologically in time is known as a Time Series(TS). A time-series dataset
is different from other datasets, as it adds a time element. This added
element is both a limitation and a structure that offers an additional
source of information[@yamak2019comparison]. TS prediction is the method of forecasting
upcoming trends/patterns of the given historical dataset with temporal
features. A time series (TS) data can be break downed into trend,
seasonality and error. A trend in TS can be observed when a certain
pattern repeats on regular intervals of time due to external factors. In
many real world scenarios, either of trend or seasonality are absent.
After finding the nature of TS, various forecasting methods have to be
applied on given TS. Given the TS, it is broadly classified into 2
categories i.e. stationary and non-stationary. A series is said to be
stationary, if it does not depend on the time components like trend,
seasonality effects. Mean and variances of such series are constant with
respect to time. Stationary TS is easier to analyze and results skillful
forecasting. A TS data is said to non-stationary if it has trend,
seasonality effects in it and changes with respect to time. Statistical
properties like mean, variance, sand standard deviation also changes
with respect to time[@chimmula2020time].

## Methods

A vanilla LSTM unit is composed of a cell, an input gate, an output gate
and a forget gate. This forget gate was not initially a part of the LSTM
network, but was proposed later by [@gers2001lstm] to allow the network
to reset its state. The cell remembers values over arbitrary time
intervals and the three gates regulate the flow of information
associated with the cell[@greff2016lstm].

![](Vanilla_LSTM.png){width="1068"}

**Figure 1** Architecture of a typical vanilla LSTM
block[@van2020review].

The LSTM architecture consists of a set of recurrently connected
sub-networks, known as memory blocks. The idea behind the memory block
is to maintain its state over time and regulate the information flow
thought non-linear gating units. The Fig. 1 displays the architecture of
a vanilla LSTM block, which involves the gates, the input signal
$x^{(t)}$, the output $y^{(t)}$, the activation functions and the
peephole connections. The output of the block is recurrently connected
back to the block input and all of the gates.

We can describe how the LSTM model works in details by assuming a
network comprising N processing blocks and M inputs. The forward pass is
this recurrent neural system is described in 6 parts.

**Block input**. This step involves updating the block input component
which combines the current input $x^{(t)}$ and the output of that LSTM
unit $y^{(t-1)}$ in the last iteration. This can be done as shown below:

$$z^{(t)} = g(W_zx^{(t)} + R_zy^{(t-1)} + b_z)    -     (1) $$

where $W_z$ and $R_z$ are the weights associated with $x^{(t)}$ and
$y^{(t-1)}$ respectively while $b_z$ stands for the bias weight vector.

**Input gate**. During this step, we update the input gate that combines
the current input $x^{(t)}$, the output of that LSTM unit $y^{(t-1)}$
and the cell value $c^{(t-1)}$ in the last iteration. This can be done
as shown below:

$$i_{(t)} =\sigma(W_ix^{(t)} + R_iy^{(t-1)} + p_i.c^{(t-1)} + b_i ) -(2) $$
where '**.**' denotes point-wise multiplication of two vectors $W_i,R_i$
and $p_i$ are the weights associated with $x^{(t)}$,$y^{(t-1)}$ and
$c^{(t-1)}$ respectively while $b_i$ represents for the bias vector
associated with this component.

In previous steps, the LSTM layer determines which information should be
retained in the network's cell states $c^{(t)}$. This included the
selection of the candidate values $z^{(t)}$ that could potentially be
added to the cell states, and the activation values $i^{(t)}$ of the
input gates.

**Forget gate**. During this step, the LSTM unit determines which
information should be removed from its previous cell states $c^{(t-1)}$.
Therefore, the activation values $f^{(t)}$ of the forget gates at the
time step *t* are calculated based on the current input $x^{(t)}$, the
outputs $y^{(t-1)}$ and the state $c^{(t-1)}$ of the memory cells at the
previous time step (t-1), the peephole connections, and the bias terms
$b_f$ of the forget gates. This can be done as shown below:

$$  f_{(t)} = \sigma(W_fx^{(t)} + R_fy^{(t-1)} + p_f.c^{(t-1)} + b_f ) -(3)     $$
where $W_f,R_f$ and $p_f$ are the weights associated with
$x^{(t)}$,$y^{(t-1)}$ and $c^{(t-1)}$ respectively while $b_f$ denotes
the bias weight vector.

**Cell**. This step computes the cell values, which combines the block
input $z^{(t)}$, the input gate $i^{(t)}$ and the forget gate $f_{(t)}$
with the previous cell value. This can be done as shown below:

$$  c^{(t)} = z^{(t)}. i^{(t)} + c^{(t-1)}.f^{(t)}-(4) $$

**Output gate**. This step calculates the output gate, which combines
the current input $x^{(t)}$, the output of that LSTM unit $y^{(t-1)}$
and the cell value $c^{(t-1)}$ in the last iteration. This can be done
as shown below:

$$  o^{(t)} = \sigma(W_ox^{(t)} + R_oy^{(t-1)} + p_o.c^{(t-1)} + b_o ) -(5) $$
where $W_o, R_o$ and $p_o$ are the weights associated with
$x^{(t)}$,$y^{(t-1)}$ and $c^{(t-1)}$ respectively, while $b_o$ denoted
for the bias weight vector.

**Block output**. Finally, we calculate the block output, which combines
the current cell value $c^{(t)}$ with the current output gate value as
follows:

$$ y^{(t)} = g(c^{(t)}). o^{(t)}-(6) $$ In the above steps, $\sigma$, g
and h denote point-wise non-linear activation functions. The logistic
sigmoid $\sigma(x) = 1/(1+e^{1-x})$ is used as a gate activation
function, while the hyperbolic tangent $g(x)= h(x)= tanh(x)$ is often
used as the block input and output activation function[@van2020review].

## Analysis and Results

**About Dataset** This dataset provides data from **1st January 2013**
to **24th April 2017** in the city of Delhi, India. The 1 parameter here
under consideration is **meantemp**

### Data and Visualization

```{r}
library(tidyverse) # importing, cleaning, visualising 
# library(tidytext) # working with text
library(keras) # deep learning with keras
library(ggplot2)
mean_temp <- read.csv("DailyDelhiMeanTemp.csv")


```

First five observations of the dataset are

```{r}
mean_temp %>% head()
```

The time series plot of the dataset is given below with **date** on
x-axis and **meantemp** on y-axis respectively.

```{r}
mean_temp$date <- as.Date(mean_temp$date, format = "%m/%d/%Y")

mean_temp$meantemp <- as.numeric(mean_temp$meantemp)


ggplot(mean_temp, aes(x = date, y = meantemp)) +
  geom_line() +
  labs(x = "Date", y = "Mean Temperature") +
  ggtitle("Mean Temperature Over Time") +
  theme(plot.title = element_text(hjust = 0.5))
```

Performing Min Max Transformations on the dataset

```{r}
mean_temp$order=seq(1, length(mean_temp$date))

# Perform min-max transformation on 'meantemp' column and store in a new column 'transformed'
mean_temp$transformed <- (mean_temp$meantemp - min(mean_temp$meantemp)) / (max(mean_temp$meantemp) - min(mean_temp$meantemp))

```

### Statistical Modeling

```{r}
library(TSLSTM)
# 
 df <- mean_temp
# 
 df$date<-as.Date(df$date)
# 
 TSLSTM<-ts.lstm(ts=df$transformed,
                 tsLag=5,
                 LSTMUnits=7,
                 DropoutRate = 0.1,
                 Epochs = 20,
                 CompLoss = "mse",
                 CompMetrics = "mae",
                 ActivationFn = "tanh",
                 SplitRatio = 0.99,
                 ValidationSplit = 0.2)
# 
# #Return function
 trainFittedValue <- TSLSTM$TrainFittedValue
 testPredictedValue <- TSLSTM$TestPredictedValue
 accuracyTable <- TSLSTM$AccuracyTable
# 
 Result<-tail(df,15)
# 
 Result$S_pred<-testPredictedValue
# 
 Result$Prediction= Result$S_pred * ( max(df$meantemp) - min(df$meantemp) ) + min(df$meantemp)
```

Plotting the Actual vs Predicted values in one visualization.

```{r}
# Calculate label positions
label_pos <- data.frame(label = c("Original", "Prediction"),
                        y = c(max(Result$meantemp), max(Result$Prediction)),
                        label_color = c("blue", "red"))

ggplot(data = Result) +
  geom_line(aes(x = date, y = meantemp), color = "blue") +
  geom_line(aes(x = date, y = Prediction), color = "red") +
  labs(x = "Date", y = "Mean Temperature (°C)",  # Updated y-axis label with units
       title = "Original Temperature Over Time") +  # Plot title
  annotate("text", x = max(Result$date) + 1, y = label_pos$y, 
           label = label_pos$label, color = label_pos$label_color, vjust = 0.5) +
  theme(axis.text.y = element_text(size = 12),   # Set font size for y-axis tick labels
        axis.title.y = element_text(size = 14),  # Set font size for y-axis label
        axis.text.x = element_text(size = 12),   # Set font size for x-axis tick labels
        axis.title.x = element_text(size = 14),  # Set font size for x-axis label
        plot.title = element_text(size = 16, hjust = 0.5))  # Set font size for plot title
```

### Conclusion

### Useful Links

## References
